{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PROGETTO DI MACHINE LEARNING**\n",
        "\n",
        "TITOLO: REGRESSIONE POLINOMIALE CON L'USO DI K-MEANS\n",
        "\n",
        "L'obiettivo del progetto è quello di sperimentare un metodo per ottimizzare il costo computazionale di un fit polinomiale, al costo di una perdita ridotta di precisione, sfruttando un prima fase in cui si utilizza k-means per ridurre i punti del dataset.\n",
        "\n",
        "L'idea sfrutta delle ricerche relativamente superficiali (utilizzando chat-gpt e wikipedia) secondo le quali k-means ha una complessità attesa inferiore ristpetto ad un fit polinomiale se applicati ad un input di n punti con k prossimo al grado del fit polinomiale. Una volta deciso il grado, quindi prossimo a k, si procede ad effettuare k-means sugli n punti iniziali, ottenendo k cluster e, con tempo lineare sul prodotto (n*d), dove d è il numero di dimensioni dell'input, ad ottenere i k baricentri dei cluster; a questo punto si effettua una fit polinomiale su questi baricentri invece che sugli n punti iniziali. L'idea si basa su quella alla base del clustering: dato che ci si aspetta come evento ragionevole che la distanze tra i punti ed i baricentri dei rispettivi cluster di appartenenza siano relativamente ridotte, si suppone di avere una perdita di accuratezza del polynomial fit regionevolmente contenuta, a fronte, per alcuni input, di un importante risparmio in termini di complessità.\n",
        "\n",
        "Nel seguito ci addentreremo meglio nei dettagli dell'idea in questione, cercando di verificarne(o smentirla) la validità e l'efficacia.\n",
        "\n",
        "Autori: Federico Ventura e Francesco Petrangeli matricole 2025655 e 1991302\n",
        "\n",
        "Iniziamo!"
      ],
      "metadata": {
        "id": "-9PfHHREu51a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Importiamo le librerie:*"
      ],
      "metadata": {
        "id": "ad4wwkODyswf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from numpy.polynomial.polynomial import Polynomial\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import silhouette_score\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "n2a-Qsahzh8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importiamo il primo datset"
      ],
      "metadata": {
        "id": "s9q2sIbr8vE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel corso del progetto utilizzeremo diversi dataset(di cui due costruiti da noi e due presi da scikit-learn), il più possibilmente reali e con caratteristiche diverse, per misurare al meglio la performance del modello su un problema vero con dati veri (e anche per prendere un bel voto:) )"
      ],
      "metadata": {
        "id": "7asxupBK1iLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Data\n",
        "data = np.loadtxt('population_over_time.dat')"
      ],
      "metadata": {
        "id": "NBnH1e_N0b_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il dataset \"**Poulation over time**\" è un dataset costruito da noi che descrive l'evolversi di una popolazione lungo un certo periodo di tempo(in anni).\n",
        "\n",
        "La spiegazione dettagliata del dataset(con relativo codice) è presente su un file che manderemo in allegato per email."
      ],
      "metadata": {
        "id": "i1hhKD0A0dap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splittiamo i dati in features e target\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]"
      ],
      "metadata": {
        "id": "WiQ4VSDm1mSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, y.shape)\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "PeiKLEJM2etK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splittiamo adesso il nostro dataset in dati di training e data di test, dato che poi applicheremo k-means sul solo training set."
      ],
      "metadata": {
        "id": "nazRyr_m49e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splittiamo il Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Uhdr3wzu5SqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "W2Ryk107-vf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniziamo adesso ad addentrarci nell'idea del progetto, applicando l'algoritmo K-means, che divide il dataset in cluster e per ogni cluster identifica un \"**centroide**\", ossia un punto medio.\n",
        "\n",
        "Saranno proprio questi centroidi i nuovi punti su cui addestreremo il modello di regressione polinomiale."
      ],
      "metadata": {
        "id": "0WIQq7lj_bPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applichiamo K-Means\n",
        "k = 15\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# prendiamo i centroidi\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Prediciamo i cluster labels per i dati di training\n",
        "labels = kmeans.predict(X_train)\n",
        "\n",
        "# prendiamo i valori target dei centroidi facendo la media dei valori target in ogni cluster\n",
        "y_centroids = np.array([y_train[labels == i].mean() for i in range(k)])\n",
        "\n",
        "# Sostituiamo il training set con i centroidi\n",
        "X_train_centroids = centroids\n",
        "y_train_centroids = y_centroids\n",
        "\n",
        "# Calcoliamo e stampiamo il punteggio silhouette\n",
        "silhouette_avg = silhouette_score(X_train, kmeans.labels_)\n",
        "print(f'Punteggio silhouette per k={k}: {silhouette_avg}')"
      ],
      "metadata": {
        "id": "5xI7u1HZ9B2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dopo aver applicato k-means abbiamo adesso ottenuto un \"**nuovo dataset**\", ossia un dataset formato esclusivamente dai centroidi, su cui adesso applicheremo regressione polinomiale."
      ],
      "metadata": {
        "id": "P8Lx8avg1ARZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione polinomiale\n",
        "\n"
      ],
      "metadata": {
        "id": "_xk7ERSp0Pre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siamo finalmente arrivati al momento di applicare regressione polinomiale!\n",
        "\n",
        "Applichiamo dunque adesso regressione polinomiale sul dataset formato dai centroidi. Facendo ciò stiamo risparmiando in termini di costo computazionale al costo di una piccola perdita di precisione."
      ],
      "metadata": {
        "id": "LK-wAD8_0urP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2em2L1736CN2"
      },
      "outputs": [],
      "source": [
        "degree = 5\n",
        "poly_model_centroids = Polynomial.fit(X_train_centroids.flatten(), y_train_centroids, degree)\n",
        "\n",
        "# Fittiamo un modello polinomiale sul training set originale\n",
        "poly_model_original = Polynomial.fit(X_train.flatten(), y_train, degree)\n",
        "\n",
        "# valutiamo i modelli\n",
        "y_pred_test_centroids = poly_model_centroids(X_test.flatten())\n",
        "y_pred_test_original = poly_model_original(X_test.flatten())\n",
        "\n",
        "# Calcoliamo R^2 sul test set per entrambi i modelli\n",
        "r2_test_centroids = r2_score(y_test, y_pred_test_centroids)\n",
        "r2_test_original = r2_score(y_test, y_pred_test_original)\n",
        "\n",
        "# Calcoliamo l'MSE sul test set per entrambi\n",
        "mse_test_centroids = mean_squared_error(y_test, y_pred_test_centroids)\n",
        "mse_test_original = mean_squared_error(y_test, y_pred_test_original)\n",
        "\n",
        "print(f'R^2 on test data (centroids): {r2_test_centroids}')\n",
        "print(f'R^2 on test data (original): {r2_test_original}')\n",
        "print(f'MSE on test data (centroids): {mse_test_centroids}')\n",
        "print(f'MSE on test data (original): {mse_test_original}')\n",
        "\n",
        "# Plottiamo i risultati\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Generiamo un range di valori di x per plottare i polinomi\n",
        "x_range = np.linspace(X_test.min(), X_test.max(), 500)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
        "plt.plot(x_range, poly_model_centroids(x_range.flatten()), color='red', label='Polynomial Fit (Centroids)', linewidth=3)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Fit on Test Data (Centroids)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
        "plt.plot(x_range, poly_model_original(x_range.flatten()), color='yellow', label='Polynomial Fit (Original)', linewidth=3)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Fit on Test Data (Original)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il risultato evidenzia come, al costo di una minima perdita di precisione, l'idea di ridurre attraverso k-means il quantitativo di dati su cui applicare regressione sia un'idea molto efficiente."
      ],
      "metadata": {
        "id": "GQ8m0Uke-brT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importiamo il secondo dataset"
      ],
      "metadata": {
        "id": "qCMPtPAuArRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing"
      ],
      "metadata": {
        "id": "UinErPOz88Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il dataset \"California Housing\" di scikit-learn contiene dati relativi ai prezzi delle case in California basati sul censimento del 1990. Questo dataset è spesso utilizzato per problemi di regressione, dove l'obiettivo è prevedere il valore medio delle case in diverse aree della California in base a una serie di caratteristiche."
      ],
      "metadata": {
        "id": "0Jzbmwka1FUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=fetch_california_housing()\n",
        "X,y = data.data, data.target"
      ],
      "metadata": {
        "id": "mT0GguCIEpZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, y.shape)\n",
        "print(X)\n",
        "print(y)\n",
        "print(data.feature_names)"
      ],
      "metadata": {
        "id": "aliOc-DrExg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il dataset contiene 20640 samples(case), ognuna con le seguenti caratteristiche (features):\n",
        "\n",
        "1 MedInc: Reddito medio nell'area (in decine di migliaia di dollari).\n",
        "\n",
        "2 HouseAge: Età media delle case in quell'area.\n",
        "\n",
        "3 AveRooms: Numero medio di stanze per abitazione.\n",
        "\n",
        "4 AveBedrms: Numero medio di camere da letto per abitazione.\n",
        "\n",
        "5 Population: Popolazione dell'area.\n",
        "\n",
        "6 AveOccup: Numero medio di occupanti per abitazione.\n",
        "\n",
        "7 Latitude: Latitudine dell'area.\n",
        "\n",
        "8 Longitude: Longitudine dell'area.\n",
        "\n",
        "La variabile target (y) è il MedHouseVal: valore medio delle case nell'area (in centinaia di migliaia di dollari)."
      ],
      "metadata": {
        "id": "5vK1m7Ag5-gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniziamo facendo un'analisi esplorativa del dataset, per cerccare di capire la distribuzione e la correlazione tra le caratteristiche del dataset in questione."
      ],
      "metadata": {
        "id": "HXAJC6SM9UYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il dataset California Housing\n",
        "california_housing = fetch_california_housing()\n",
        "df = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "df['MedHouseVal'] = california_housing.target\n",
        "\n",
        "# Visualizza le prime righe del dataset\n",
        "print(df.head())\n",
        "\n",
        "# Descrizione statistica delle caratteristiche numeriche\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "# Pairplot per visualizzare le relazioni tra le caratteristiche\n",
        "sns.pairplot(df, diag_kind='kde')\n",
        "plt.suptitle('Pairplot delle caratteristiche del dataset California Housing', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Heatmap delle correlazioni\n",
        "plt.figure(figsize=(12, 8))\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Heatmap delle correlazioni tra le caratteristiche')\n",
        "plt.show()\n",
        "\n",
        "# Scatterplot di alcune caratteristiche rispetto al valore mediano delle case\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "sns.scatterplot(data=df, x='MedInc', y='MedHouseVal', ax=axs[0, 0])\n",
        "axs[0, 0].set_title('MedInc vs MedHouseVal')\n",
        "\n",
        "sns.scatterplot(data=df, x='AveRooms', y='MedHouseVal', ax=axs[0, 1])\n",
        "axs[0, 1].set_title('AveRooms vs MedHouseVal')\n",
        "\n",
        "sns.scatterplot(data=df, x='AveOccup', y='MedHouseVal', ax=axs[1, 0])\n",
        "axs[1, 0].set_title('AveOccup vs MedHouseVal')\n",
        "\n",
        "sns.scatterplot(data=df, x='AveBedrms', y='MedHouseVal', ax=axs[1, 1])\n",
        "axs[1, 1].set_title('AveBedrms vs MedHouseVal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VdzQq3sW9nw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dai grafici appena plottati si evidenzia che \"Medinc\", ossia il valore medio di reddito, è la caratteristica maggiormente correlata con il target. Inoltre si evidenzia che la caratteristica \"Aveoccup\" sembra essere poco rilevante al fine di predire il valore di una casa, dunque può essere rimossa al fine di ridurre la dimensionalità del dataset.\n",
        "Infine le caratteristiche \"AveBedrms\" e \"AveRooms\" risultano essere altamente correlate e dunque se ne può eliminare una delle due per ridurre la multicolinearità."
      ],
      "metadata": {
        "id": "m8_ipmKVG0It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rimuoviamo le due colonne dalla matrice del dataset\n",
        "colonne_da_eliminare=[3,5]\n",
        "X= np.delete(X,colonne_da_eliminare,axis=1)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "z690ffV6yDiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dopo aver fatto feature selection, cerchiamo di ridurre ulteriormente la dimensionalità del dataset(che risulta ancora alta dato che ogni sample è 6-dimensionale.\n",
        "\n",
        "Proviamo adesso ad applicare PCA e prendiamo le prime tre componenti principali,dopo aver però verificato che esse contengono la maggior parte dell'informazione del dataset."
      ],
      "metadata": {
        "id": "3XNp1T4uvVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#standardizziamo le caratteristiche\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "2sjsvA-nydj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applica PCA per ridurre a 3 dimensioni\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Varianza spiegata dalle prime 3 componenti principali\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "total_explained_variance = np.sum(explained_variance_ratio)\n",
        "\n",
        "print(f'Varianza spiegata da ciascuna delle prime 3 componenti principali: {explained_variance_ratio}')\n",
        "print(f'Varianza totale spiegata dalle prime 3 componenti principali: {total_explained_variance}')\n",
        "\n",
        "# Visualizzazione della varianza spiegata\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, 4), explained_variance_ratio, alpha=0.5, align='center', label='Singola varianza')\n",
        "plt.step(range(1, 4), np.cumsum(explained_variance_ratio), where='mid', label='Varianza cumulativa')\n",
        "plt.xlabel('Componenti principali')\n",
        "plt.ylabel('Varianza')\n",
        "plt.title('Varianza dalle prime 3 componenti principali')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rACh-0WoyvT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da tali risultati si evince che le prime tre componenti principali catturano la maggior parte dell'informazione del dataset. Risulta dunque utile e funzionale in questo caso applicare PCA."
      ],
      "metadata": {
        "id": "0crd6E3ozLu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splittiamo il dataset in training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-JJRdErpHXoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "gsJvOxEOHgGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniziamo adesso ad addentrarci nell'idea del progetto, applicando l'algoritmo K-means, che divide il dataset in cluster e per ogni cluster identifica un \"**centroide**\".\n",
        "\n",
        "Saranno proprio questi centroidi i nuovi punti su cui addestreremo un modello di regressione polinomiale."
      ],
      "metadata": {
        "id": "zwJFOBUjQu89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combiniamo X_train e y_train in un solo dataset\n",
        "train_combined = np.c_[X_train, y_train]"
      ],
      "metadata": {
        "id": "ll8wETCuQ0Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applichiamo K-Means\n",
        "k = 2500\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(train_combined)\n",
        "\n",
        "# Sostituiamo il training set con i centroidi\n",
        "centroids = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Splittiamo i centroidi in features e targets\n",
        "X_train_reduced = centroids[:, :-1]\n",
        "y_train_reduced = centroids[:, -1]\n",
        "\n",
        "# Visualizza i primi 5 campioni con i cluster assegnati\n",
        "for i in range(5):\n",
        "    print(f\"Campione {i+1}:\")\n",
        "    print(f\"  Caratteristiche: {X[i]}\")\n",
        "    print(f\"  Valore Mediano della Casa: {y[i]}\")\n",
        "    print(f\"  Cluster Assegnato: {labels[i]}\\n\")\n",
        "\n",
        "# Calcola e stampa il punteggio silhouette\n",
        "silhouette_avg = silhouette_score(train_combined, kmeans.labels_)\n",
        "print(f'Punteggio silhouette per k={k}: {silhouette_avg}')"
      ],
      "metadata": {
        "id": "mU6aJy39Q3dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione polinomiale\n",
        "\n"
      ],
      "metadata": {
        "id": "HNbQDPU4Q87n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fittiamo un modello sul training set formato dai centroidi\n",
        "degree = 60\n",
        "X_train_reduced_1d = X_train_reduced[:, 0]\n",
        "coefs_reduced = np.polyfit(X_train_reduced_1d, y_train_reduced, degree)\n",
        "polyfit_model_reduced = np.poly1d(coefs_reduced)\n",
        "\n",
        "# Fittiamo un modello sul training set originale\n",
        "X_train_1d = X_train[:, 0]\n",
        "coefs_original = np.polyfit(X_train_1d, y_train, degree)\n",
        "polyfit_model_original = np.poly1d(coefs_original)\n",
        "\n",
        "# valutiamo sul test set entrambi i modelli\n",
        "X_test_1d = X_test[:, 0]\n",
        "y_pred_reduced = polyfit_model_reduced(X_test_1d)\n",
        "y_pred_original = polyfit_model_original(X_test_1d)\n",
        "\n",
        "# Calculiamo l'MSE\n",
        "mse_reduced = mean_squared_error(y_test, y_pred_reduced)\n",
        "mse_original = mean_squared_error(y_test, y_pred_original)\n",
        "\n",
        "# Calculiamo R²\n",
        "r2_reduced = r2_score(y_test, y_pred_reduced)\n",
        "r2_original = r2_score(y_test, y_pred_original)\n",
        "\n",
        "print(\"Mean Squared Error on Test Set (Reduced):\", mse_reduced)\n",
        "print(\"R² Score on Test Set (Reduced):\", r2_reduced)\n",
        "\n",
        "print(\"Mean Squared Error on Test Set (Original):\", mse_original)\n",
        "print(\"R² Score on Test Set (Original):\", r2_original)"
      ],
      "metadata": {
        "id": "pqUaP2HwRA3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non sembra un buon risultato :( , proviamo dunque a ripetere lo stesso procedimento ma senza utilizzare PCA."
      ],
      "metadata": {
        "id": "tapz2xzRRInv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splittiamo il dataset in training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ufETgqANQmBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "8gvUuYgBQn8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combiniamo X_train e y_train in un solo dataset\n",
        "train_combined = np.c_[X_train, y_train]"
      ],
      "metadata": {
        "id": "KESqqt8YjN_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applichiamo K-means\n",
        "k = 2500\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(train_combined)\n",
        "\n",
        "# Sostituiamo il training set con i centroidi\n",
        "centroids = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Splittiamo i centroidi in features e targets\n",
        "X_train_reduced = centroids[:, :-1]\n",
        "y_train_reduced = centroids[:, -1]\n",
        "\n",
        "# Visualizza i primi 5 campioni con i cluster assegnati\n",
        "for i in range(5):\n",
        "    print(f\"Campione {i+1}:\")\n",
        "    print(f\"  Caratteristiche: {X[i]}\")\n",
        "    print(f\"  Valore Mediano della Casa: {y[i]}\")\n",
        "    print(f\"  Cluster Assegnato: {labels[i]}\\n\")\n",
        "\n",
        "# Calcola e stampa il punteggio silhouette\n",
        "silhouette_avg = silhouette_score(train_combined, kmeans.labels_)\n",
        "print(f'Punteggio silhouette per k={k}: {silhouette_avg}')"
      ],
      "metadata": {
        "id": "r6KxDBEjIByA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione polinomiale\n",
        "\n"
      ],
      "metadata": {
        "id": "WXFtkal7Iz7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fittiamo un modello sul training set formato dai centroidi\n",
        "degree = 60\n",
        "X_train_reduced_1d = X_train_reduced[:, 0]\n",
        "coefs_reduced = np.polyfit(X_train_reduced_1d, y_train_reduced, degree)\n",
        "polyfit_model_reduced = np.poly1d(coefs_reduced)\n",
        "\n",
        "# Fittiamo un modello sul training set oroginale\n",
        "X_train_1d = X_train[:, 0]\n",
        "coefs_original = np.polyfit(X_train_1d, y_train, degree)\n",
        "polyfit_model_original = np.poly1d(coefs_original)\n",
        "\n",
        "# Valutiamo entrambi i modelli sul test set\n",
        "X_test_1d = X_test[:, 0]\n",
        "y_pred_reduced = polyfit_model_reduced(X_test_1d)\n",
        "y_pred_original = polyfit_model_original(X_test_1d)\n",
        "\n",
        "# Calcoliamo l'MSE\n",
        "mse_reduced = mean_squared_error(y_test, y_pred_reduced)\n",
        "mse_original = mean_squared_error(y_test, y_pred_original)\n",
        "\n",
        "# Calcoliamo R²\n",
        "r2_reduced = r2_score(y_test, y_pred_reduced)\n",
        "r2_original = r2_score(y_test, y_pred_original)\n",
        "\n",
        "print(\"Mean Squared Error on Test Set (Reduced):\", mse_reduced)\n",
        "print(\"R² Score on Test Set (Reduced):\", r2_reduced)\n",
        "\n",
        "print(\"Mean Squared Error on Test Set (Original):\", mse_original)\n",
        "print(\"R² Score on Test Set (Original):\", r2_original)"
      ],
      "metadata": {
        "id": "TovAW8gDAvwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ecco che qui abbiamo un risultato molto più soddisfacente: infatti l'MSE risulta essere molto basso e l'R^2 ha un valore buono(vicino a 0.5).\n",
        "\n",
        "Inoltre possiamo vedere come si abbia una differenza minima tra i risultati ottenuti con la classica regressione polinomiale e quella fatta con i centroidi."
      ],
      "metadata": {
        "id": "olmbLd90RkeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importiamo il terzo dataset"
      ],
      "metadata": {
        "id": "88ceg5Bl0_XL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il dataset \"**Epidemy diffusion**\" è un dataset costruito da noi che descrive l'evolversi di un'epidemia di una popolazione lungo un certo periodo di tempo(in anni).\n",
        "\n",
        "La spiegazione dettagliata del dataset(con relativo codice) è presente su un file che manderemo in allegato per email."
      ],
      "metadata": {
        "id": "VV6_isLU36Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Data\n",
        "data = np.loadtxt('epidemy_diffusion_3.0.dat')\n",
        "\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "8kdZraIX4gIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splittiamo il dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ZfpI4bFw6jY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "Go7DHyNJ6wbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applichiamo K-Means\n",
        "k = 80\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "labels = kmeans.predict(X_train)\n",
        "\n",
        "# prendiamo i valori target dei centroidi facendo la media dei valori target in ogni cluster\n",
        "y_centroids = np.array([y_train[labels == i].mean() for i in range(k)])\n",
        "\n",
        "\n",
        "# Sostituiamo il training Set con i centroidi\n",
        "X_train_centroids = centroids\n",
        "y_train_centroids = y_centroids\n",
        "\n",
        "\n",
        "# Calcola e stampa il punteggio silhouette\n",
        "silhouette_avg = silhouette_score(X_train, kmeans.labels_)\n",
        "print(f'Punteggio silhouette per k={k}: {silhouette_avg}')"
      ],
      "metadata": {
        "id": "3TTKCAFP7gQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione polinomiale"
      ],
      "metadata": {
        "id": "y220ERxG7pY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  fit polinomiale sui centroidi\n",
        "degree = 25\n",
        "poly_model_centroids = Polynomial.fit(X_train_centroids.flatten(), y_train_centroids, degree)\n",
        "\n",
        "# fit sul training set originale\n",
        "poly_model_original = Polynomial.fit(X_train.flatten(), y_train, degree)\n",
        "\n",
        "# valutiamo i modelli\n",
        "y_pred_test_centroids = poly_model_centroids(X_test.flatten())\n",
        "y_pred_test_original = poly_model_original(X_test.flatten())\n",
        "\n",
        "# R^2\n",
        "r2_test_centroids = r2_score(y_test, y_pred_test_centroids)\n",
        "r2_test_original = r2_score(y_test, y_pred_test_original)\n",
        "\n",
        "# MSE\n",
        "mse_test_centroids = mean_squared_error(y_test, y_pred_test_centroids)\n",
        "mse_test_original = mean_squared_error(y_test, y_pred_test_original)\n",
        "\n",
        "# Print dei risultati\n",
        "print(f'\\nR^2 on test data (centroids): {r2_test_centroids}')\n",
        "print(f'R^2 on test data (original): {r2_test_original}')\n",
        "print(f'MSE on test data (centroids): {mse_test_centroids}')\n",
        "print(f'MSE on test data (original): {mse_test_original}')\n",
        "\n",
        "# Plottiamo il training set e i centroidi\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(X_train, y_train, color='green', label='Original Training Set', s=5)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Original Training Set')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(X_train_centroids, y_train_centroids, color='red', label='Centroids Set', s=10)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Centroids Set')\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "x_range = np.linspace(X_test.min(), X_test.max(), 500)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual Data', s=20)\n",
        "plt.plot(x_range, poly_model_centroids(x_range.flatten()), color='red', linewidth=2.5, label='Polynomial Fit (Centroids)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Fit on Test Data (Centroids)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual Data', s=20)\n",
        "plt.plot(x_range, poly_model_original(x_range.flatten()), color='green', linewidth=2.5, label='Polynomial Fit (Original)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Fit on Test Data (Original)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hTlo4LNL1Fov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questo caso ,utilizzando il fit con i centroidi rispetto al classico, si nota addirittura un piccolissimo guadagno in termini di precisione, come evidenziato dai valori MSE e R^2"
      ],
      "metadata": {
        "id": "a2U4Qzem_9As"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importiamo il quarto dataset"
      ],
      "metadata": {
        "id": "m1IEbUmYXMON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_linnerud"
      ],
      "metadata": {
        "id": "9EGoI649XSo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il dataset \"**Linnerud**\" di scikit-learn è un dataset multidimensionale che contiene misurazioni relative all'esercizio fisico e ai parametri fisiologici."
      ],
      "metadata": {
        "id": "YiToj96UX_q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caratteristiche (Features):\n",
        "\n",
        "Il dataset include 3 caratteristiche che misurano l'attività fisica dei soggetti:\n",
        "\n",
        "1. Chins: Numero di flessioni alla sbarra (pull-ups).\n",
        "\n",
        "2. Sit-ups: Numero di addominali eseguiti in 60 secondi.\n",
        "\n",
        "3. Jumps: Distanza massima di salto in centimetri.\n",
        "\n",
        "Queste caratteristiche sono indicazioni delle capacità fisiche e della forma fisica generale dei soggetti.\n",
        "\n",
        "Target:\n",
        "\n",
        "Il dataset include 3 variabili target che rappresentano parametri fisiologici dei soggetti:\n",
        "\n",
        "1. Weight: Peso del soggetto in kg.\n",
        "\n",
        "2. Waist: Circonferenza della vita in cm.\n",
        "\n",
        "3. Pulse: Frequenza cardiaca a riposo in battiti per minuto (bpm).\n",
        "\n",
        "Queste variabili target sono misure fisiologiche che possono essere influenzate dal livello di attività fisica del soggetto."
      ],
      "metadata": {
        "id": "cSwOt0JBc4HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_linnerud()\n",
        "X = data.data\n",
        "y = data.target[:, 0]  # Consideriamo solo il peso come target per semplicità\n",
        "print(X.shape,y.shape)"
      ],
      "metadata": {
        "id": "tz9-lc6Ud6Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adesso cerchiamo di ridurre la dimensionalità del dataset a 2 dimensioni, attraverso PCA e previa analisi della varianza contenuta  nelle prime due componenti principali, in modo da poter poi visualizzare in 3D il fit polinomiale(per questo abbiamo il target ad una sola componente)"
      ],
      "metadata": {
        "id": "jBmBjGTkeRB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applicare PCA per ridurre le dimensioni a 2\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# Verificare quanta varianza contengono le prime due componenti principali\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f'Varianza spiegata dalle prime due componenti principali: {explained_variance}')# Dividere il dataset in training e test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "QU-QU72Ye8cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come si vede dunque le prime due componenti principali catturano la quasi totalità dell'informazione del dataset."
      ],
      "metadata": {
        "id": "tp1Fq7JvfZoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividiamo il dataset in training e test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "U2ffuOIVgP9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "uqxpvG7MgXqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_combined = np.c_[X_train, y_train]"
      ],
      "metadata": {
        "id": "1AjfIzANjTXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(train_combined)\n",
        "\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "X_train_reduced = centroids[:, :-1]\n",
        "y_train_reduced = centroids[:, -1]\n",
        "\n",
        "silhouette_avg = silhouette_score(train_combined, kmeans.labels_)\n",
        "print(f'Punteggio silhouette per k={k}: {silhouette_avg}')"
      ],
      "metadata": {
        "id": "CRGb6zM2j6X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione polinomiale"
      ],
      "metadata": {
        "id": "dZVtuegtlwDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trasformiamo le caratteristiche in caratteristiche polinomiali\n",
        "degree = 2  # Grado del polinomio\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly.fit_transform(X_train_reduced)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# Applicare la regressione lineare sulle caratteristiche polinomiali\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_poly, y_train_reduced)\n",
        "\n",
        "# Predire i valori\n",
        "y_train_pred = model.predict(X_train_poly)\n",
        "y_test_pred = model.predict(X_test_poly)\n",
        "\n",
        "# Calcolare l'MSE e l'R^2 per i dati di training\n",
        "train_mse = mean_squared_error(y_train_reduced, y_train_pred)\n",
        "train_r2 = r2_score(y_train_reduced, y_train_pred)\n",
        "\n",
        "# Calcolare l'MSE e l'R^2 per i dati di test\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Train MSE: {train_mse}')\n",
        "print(f'Train R^2: {train_r2}')\n",
        "print(f'Test MSE: {test_mse}')\n",
        "print(f'Test R^2: {test_r2}')\n",
        "\n",
        "# Visualizzare il fit polinomiale sui punti in un grafico 3D interattivo con Plotly\n",
        "# Punti originali\n",
        "scatter = go.Scatter3d(\n",
        "    x=X_reduced[:, 0],\n",
        "    y=X_reduced[:, 1],\n",
        "    z=y,\n",
        "    mode='markers',\n",
        "    marker=dict(size=5, color='blue', opacity=0.8),\n",
        "    name='Dati originali'\n",
        ")\n",
        "\n",
        "# Creare una griglia di punti per visualizzare il fit polinomiale\n",
        "x0_range = np.linspace(X_reduced[:, 0].min(), X_reduced[:, 0].max(), 100)\n",
        "x1_range = np.linspace(X_reduced[:, 1].min(), X_reduced[:, 1].max(), 100)\n",
        "x0_grid, x1_grid = np.meshgrid(x0_range, x1_range)\n",
        "x_grid = np.c_[x0_grid.ravel(), x1_grid.ravel()]\n",
        "x_grid_poly = poly.transform(x_grid)\n",
        "y_grid_pred = model.predict(x_grid_poly).reshape(x0_grid.shape)\n",
        "\n",
        "# Superficie del fit polinomiale\n",
        "surface = go.Surface(\n",
        "    x=x0_grid,\n",
        "    y=x1_grid,\n",
        "    z=y_grid_pred,\n",
        "    colorscale='Viridis',\n",
        "    opacity=0.5,\n",
        "    name='Superficie del fit polinomiale'\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Fit Polinomiale sui Dati Linnerud',\n",
        "    scene=dict(\n",
        "        xaxis_title='Prima componente principale',\n",
        "        yaxis_title='Seconda componente principale',\n",
        "        zaxis_title='Peso'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[scatter, surface], layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TFc_6rLYlxkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come si evince da questi risultati il modello fitta benissimo i dati di training dato che il grado del polinomio è >= (n_cluster - 1), ma ha performance molto scadenti sul test set. Si ha dunque un overfitting, a causa della ridotta quantità di punti presenti nel dataset.\n",
        "\n",
        "Ecco dunque un esempio in cui l'idea di utilizzare k-means non produce buoni risultati. Fortunamente però il campo di applicazione previsto per questa procedura è costituito da dataset estremamente grandi, come si è potuto constatare in precedenza."
      ],
      "metadata": {
        "id": "vMXvbPBmoS_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generiamo l'ultimo dataset: una curva"
      ],
      "metadata": {
        "id": "4shDgQUOu8RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come ultimo dataset utilizzeremo dei punti generati dalla seguente curva,detta curva di Sine, definita come:\n",
        "              \n",
        "$$f(t) = (t, \\sin(t), \\sin(2t))\\,,\\quad\\quad t \\in [0, 5\\pi]$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "De_toZ57vFNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.linspace(0, 2 * np.pi, 100)\n",
        "x = t\n",
        "y = np.sin(t)\n",
        "z = np.sin(2 * t)\n",
        "\n",
        "# Plot della curva in R^3\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.plot(x, y, z, label='(t, sin(t), sin(2*t)', color='red', linewidth=4)\n",
        "\n",
        "ax.set_xlabel('t')\n",
        "ax.set_ylabel('sin(t)')\n",
        "ax.set_zlabel('sin(2*t)')\n",
        "\n",
        "ax.set_title('Curva (t, sin(t), sin(2*t)) in R^3')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LT5R_6M_3zcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reatà però quello che faremo non sarà prendere dei normali punti della curva,ma sarà prendere dei punti dalla curva perturbata."
      ],
      "metadata": {
        "id": "6DHbaYM15An8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generazione del dataset con perturbazione\n",
        "np.random.seed(0)\n",
        "t = np.linspace(0, 2 * np.pi, 100)\n",
        "x = t\n",
        "y = np.sin(t) + 0.1 * np.random.randn(100)\n",
        "z = np.sin(2 * t) + 0.1 * np.random.randn(100)\n",
        "\n",
        "# Dataset\n",
        "data = np.vstack((x, y, z)).T"
      ],
      "metadata": {
        "id": "sf27Et6Cw6Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adesso splittiamo il dataset in training set e test set"
      ],
      "metadata": {
        "id": "OUS6I4DUyRJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisione del dataset in training e test set\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "3To_Q0rEyXEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means"
      ],
      "metadata": {
        "id": "8z8U2Eau5S4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applichiamo k-means\n",
        "kmeans = KMeans(n_clusters=10, random_state=0).fit(train_data)\n",
        "train_labels = kmeans.labels_\n",
        "test_labels = kmeans.predict(test_data)\n",
        "\n",
        "# prendiamo i centroidi\n",
        "centroids = kmeans.cluster_centers_\n"
      ],
      "metadata": {
        "id": "vbDEYX0l5Wwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adesso applicheremo regressione polinomiale sul dataset originale e su quello formato dai centroidi, cercando di cogliere le differenze tra i due modelli"
      ],
      "metadata": {
        "id": "XMWGIzbW572L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressione polinomiale"
      ],
      "metadata": {
        "id": "PHHGPlxU6Wnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regressione polinomiale sui centroidi\n",
        "poly = PolynomialFeatures(degree=5)\n",
        "centroids_poly = poly.fit_transform(centroids[:, :2])\n",
        "\n",
        "model_centroids = LinearRegression()\n",
        "model_centroids.fit(centroids_poly, centroids[:, 2])\n",
        "\n",
        "# Predizione sui dati di test utilizzando il modello basato sui centroidi\n",
        "X_test_poly = poly.transform(test_data[:, :2])\n",
        "z_test_pred_centroids = model_centroids.predict(X_test_poly)\n",
        "\n",
        "# Regressione polinomiale diretta\n",
        "X_train_poly = poly.fit_transform(train_data[:, :2])\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_poly, train_data[:, 2])\n",
        "z_test_pred = model.predict(X_test_poly)\n",
        "\n",
        "# MSE e R^2 per regressione polinomiale diretta e con centroidi\n",
        "mse_direct = mean_squared_error(test_data[:, 2], z_test_pred)\n",
        "r2_direct = r2_score(test_data[:, 2], z_test_pred)\n",
        "mse_centroids = mean_squared_error(test_data[:, 2], z_test_pred_centroids)\n",
        "r2_centroids = r2_score(test_data[:, 2], z_test_pred_centroids)"
      ],
      "metadata": {
        "id": "jls3n9-c6a6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stampiamo adesso MSE e R^2 e facciamo un plot del fit di entrambi i modelli"
      ],
      "metadata": {
        "id": "a27kTses6mkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot per regressione polinomiale diretta\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.scatter(test_data[:, 0], test_data[:, 1], test_data[:, 2], c='r', marker='o', label='Dati Test Originali')\n",
        "\n",
        "# Generazione di una griglia per il fit polinomiale\n",
        "x_range = np.linspace(test_data[:, 0].min(), test_data[:, 0].max(), 100)\n",
        "y_range = np.linspace(test_data[:, 1].min(), test_data[:, 1].max(), 100)\n",
        "x_grid, y_grid = np.meshgrid(x_range, y_range)\n",
        "xy_grid = np.c_[x_grid.ravel(), y_grid.ravel()]\n",
        "z_grid = model.predict(poly.transform(xy_grid)).reshape(x_grid.shape)\n",
        "\n",
        "ax.plot_trisurf(xy_grid[:, 0], xy_grid[:, 1], model.predict(poly.transform(xy_grid)), alpha=0.5, color='b')\n",
        "ax.set_title('Regressione Polinomiale Diretta')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "\n",
        "# Plot per regressione polinomiale con centroidi\n",
        "ax = fig.add_subplot(122, projection='3d')\n",
        "ax.scatter(test_data[:, 0], test_data[:, 1], test_data[:, 2], c='r', marker='o', label='Dati Test Originali')\n",
        "\n",
        "# Generazione di una griglia per il fit polinomiale con centroidi\n",
        "ax.plot_trisurf(xy_grid[:, 0], xy_grid[:, 1], model_centroids.predict(poly.transform(xy_grid)), alpha=0.5, color='g')\n",
        "ax.set_title('Regressione Polinomiale con Centroidi')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Stampa di MSE e R^2\n",
        "print(f'MSE Regressione Polinomiale Diretta (Test): {mse_direct}')\n",
        "print(f'R^2 Regressione Polinomiale Diretta (Test): {r2_direct}')\n",
        "print(f'MSE Regressione Polinomiale con Centroidi (Test): {mse_centroids}')\n",
        "print(f'R^2 Regressione Polinomiale con Centroidi (Test): {r2_centroids}')"
      ],
      "metadata": {
        "id": "RtxiG0v56xk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anche in questo caso al costo di una piccola perdita di precisione si ottengono degli ottimi risultati, come evidenziato dal plot della superficie e dai valori di MSE e R^2."
      ],
      "metadata": {
        "id": "VFUtRvIe7JTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusioni finali"
      ],
      "metadata": {
        "id": "gwrT9LQZ8Svb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dopo aver esaminato alcuni dataset siamo giunti alla conclusione che il metodo descritto nel progetto ha un buon potenziale di applicazione, in particolare nel caso di dataset abbastanza grandi e adatti alla clusterizzazione.\n",
        "\n",
        "Speriamo con ENORME umiltà che questa(a dir poco) rivoluzionaria idea possa portare a grandi innovazioni nel campo del machine learning😂"
      ],
      "metadata": {
        "id": "VhB6jyBS8Ych"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonti: Chatgpt, scikit-learn, wikipedia, lezioni del docente E. Rodolà."
      ],
      "metadata": {
        "id": "hjiDhzksBB-6"
      }
    }
  ]
}